# -*- coding: utf-8 -*-
"""lab1_sub.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XQADk6dT6PK8jpf3YXPU7Tu0w3HaB2xG
"""

import pandas as pd
import torch
import numpy as np
import matplotlib.pyplot as plt

"""#Data processing"""

def dataProcessing(filename):
  df = pd.read_csv('beam_data_1.csv')
  df.head()
  # df = df.sample(frac=1)
  np_data = df.to_numpy()
  # print(type(np_data))
  m, n = np_data.shape[0], np_data.shape[1]
  # print(np_data)
  bias = np.exp(1)*np.ones((m,1))
  np_data = np.append(bias, np_data, axis=1)
  # print(np_data)

  m, n = np_data.shape[0], np_data.shape[1]
  print(m, n)
  train_data_size = int(m*0.8)
  train_data = np_data[0: train_data_size]
  test_data = np_data[train_data_size: ]
  # print(train_data)
  # print(type(train_data))
  # print(train_data.size)


  x_train = train_data[:, 0:n-1]
  y_train = train_data[:, -1:]
  x_test = test_data[:, 0:n-1]
  y_test = test_data[:, -1:]

  return x_train, y_train, x_test, y_test

"""
#Question 1. a. Ordinary least square method"""

def linearReg(x_train, y_train, iter, alpha=0.001):
  m, n = x_train.shape[0], x_train.shape[1]
  x_train = np.log(x_train)
  y_train = np.log(y_train)

  cost_history = []

  w = np.ones((n,1))
  # print(w.shape)

  for i in range(iter):

    #prediction
    y_hat = np.dot(x_train, w)
    y_hat = y_hat.reshape(-1,1)

    del_y = y_hat - y_train

    #cost
    cost = np.sum((del_y)**2) / (2 * m)
    # print(cost)
    cost_history.append(cost)


    #gradient
    grad = np.dot(x_train.T, (y_hat - y_train))/m
    # print('grad: ' , grad)
    w = w - alpha*grad

  plt.figure(1)
  # plt.subplot(211)
  plt.plot(cost_history)
  print(cost_history[-1])


  return w, cost_history

"""#Question 1. b. Least square algorithm with lasso regularization"""

def linearReg_lasso(x_train, y_train, iter, lmda, alpha=0.001):
  m, n = x_train.shape[0], x_train.shape[1]
  x_train = np.log(x_train)
  y_train = np.log(y_train)

  cost_history = []

  w = np.ones((n,1))
  # print(w.shape)

  for i in range(iter):

    #prediction
    y_hat = np.dot(x_train, w)
    y_hat = y_hat.reshape(-1,1)

    del_y = y_hat - y_train

    #cost
    lasso_term = lmda * np.sum(np.abs(w))
    cost = (np.sum(del_y**2) + lasso_term) / (2 * m)
    # cost = np.sum((del_y)**2) / (2 * m)
    # print(cost)
    cost_history.append(cost)


    #gradient

    grad = (np.dot(x_train.T, del_y) + lmda * np.sign(w)) /m
    # print('grad: ' , grad)
    w = w - alpha*grad

  plt.figure(4)
  # plt.subplot(211)
  plt.plot(cost_history)
  print(cost_history[-1])

  return w, cost_history

"""#Question 1. c. Least square algorithm with ridge regularization"""

def linearReg_ridge(x_train, y_train, iter, lmda,  alpha=0.001):
  m, n = x_train.shape[0], x_train.shape[1]
  x_train = np.log(x_train)
  y_train = np.log(y_train)

  cost_history = []

  w = np.ones((n,1))
  # print(w.shape)

  for i in range(iter):

    #prediction
    y_hat = np.dot(x_train, w)
    y_hat = y_hat.reshape(-1,1)

    del_y = y_hat - y_train

    #cost
    ridge_term = lmda * np.sum(w**2)
    cost = (np.sum(del_y**2) + ridge_term) / (2 * m)
    # cost = np.sum((del_y)**2) / (2 * m)
    # print(cost)
    cost_history.append(cost)


    #gradient
    grad = (np.dot(x_train.T, del_y) + lmda * w)/m
    # print('grad: ' , grad)
    w = w - alpha*grad

  plt.figure(3)
  # plt.subplot(211)
  plt.plot(cost_history)
  print(cost_history[-1])


  return w, cost_history

"""# 2. a. gradient descent using numpy library and/or pytorch libraryâ€™s"""

def linearReg_torch(x_train, y_train, iter, alpha=0.001):
  x_train = np.log(x_train)
  y_train = np.log(y_train)

  w = torch.ones([6,1]).requires_grad_()
  w = w.to(torch.float32)

  phi = torch.from_numpy(x_train)
  phi = phi.to(torch.float32)

  y_train_tensor = torch.from_numpy(y_train)
  y_train_tensor = y_train_tensor.to(torch.float32)



  cost_history = []


  for i in range(iter):
    y_pr = torch.matmul(phi, w)
    loss = torch.mean((y_pr-y_train_tensor)**2)
    # print(loss)
    # print(type(loss.item()))
    cost_history.append(loss.item())
    loss.backward()

    w.data = w.data-alpha*w.grad.data
    # b.data = b.data-0.001*b.grad.data

    w.grad.data.zero_()
    # b.grad.data.zero_()

  # print(w)
  plt.figure(2)
  # plt.subplot(212)
  plt.plot(cost_history)
  print(cost_history[-1])


  return w, cost_history

"""#Direct approach

"""

def linearReg_direct(x_train, y_train):
  x_train = np.log(x_train)
  y_train = np.log(y_train)

  phi = torch.from_numpy(x_train)
  y_train_tensor = torch.from_numpy(y_train)
  # print(phi)
  phi_t = torch.transpose(phi,0,1)
  phi_t_phi = torch.matmul(phi_t, phi)
  phi_t_phi_inv = torch.inverse(phi_t_phi)
  phi_t_phi_inv_phi_t = torch.matmul(phi_t_phi_inv, phi_t)
  w_direct = torch.matmul(phi_t_phi_inv_phi_t, y_train_tensor)

  # w_direct, w_direct.shape
  return w_direct

"""#testing dataset"""

def test(x_test, y_test, w, sample_size, p):
  x_test = np.log(x_test)
  y_test = np.log(y_test)
  y_pred = np.dot(x_test, w)
  # print(y_test.shape, y_pred.shape)

  m = y_test.shape[0]
  n = y_test.shape[1]

  del_y = y_test-y_pred
  cost = np.sum((del_y)**2) / (2 * m)
  print(cost)

  # plt.subplot(299)
  plt.figure(4+p)
  plt.plot(y_test[:sample_size])
  plt.plot(y_pred[:sample_size])

x_train, y_train, x_test, y_test = dataProcessing('beam_data_1.csv')

# def linearReg(x_train, y_train, iter, alpha=0.001):
# def linearReg_direct(x_train, y_train):
# def linearReg_torch(x_train, y_train, iter, alpha=0.001):
# def test(x_test, y_test, w, sample_size):

"""#linear reg test"""

w_lr, _  = linearReg(x_train, y_train, 1000)
test(x_test, y_test, w_lr, 20, 1)

print(w_lr)

"""#direct approach test"""

w_lr_d = linearReg_direct(x_train, y_train)
test(x_test, y_test, w_lr_d, 20, 2)

print(w_lr_d)

"""#linear reg torch test

"""

w_lr_t, _ = linearReg_torch(x_train, y_train, 1000)
test(x_test, y_test, w_lr_t.detach().numpy(), 20, 3)

print(w_lr_t)

"""#linear regression ridge reg test"""

w_lr_r, _ = linearReg_ridge(x_train, y_train, 1000, 2)
test(x_test, y_test, w_lr_r, 20, 4)

print(w_lr_r)

"""#linear regression lasso reg

"""

w_lr_l, _ = linearReg_lasso(x_train, y_train, 1000, 2)
test(x_test, y_test, w_lr_l, 20, 5)

print(w_lr_l)